# @package backbone
_target_: dalle_pytorch.DALLE
dim: 1024
text_seq_len: 77          # text sequence length
depth: 12                 # should aim to be 64
heads: 16                 # attention heads
dim_head: 64              # attention head dimension
attn_dropout: 0.1         # attention dropout
ff_dropout: 0.1
